{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACdYppHlo4Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Training code (two classifiers: Urgency + Topic)"
      ],
      "metadata": {
        "id": "EMbi9BIGsSGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import re\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------\n",
        "# 1) Minimal text cleaner\n",
        "# -----------------------\n",
        "URL_RE = re.compile(r'https?://\\S+')\n",
        "WS_RE  = re.compile(r'\\s+')\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' ', s)\n",
        "    s = s.replace('\\n', ' ')\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def combine_fields(subject: str, body: str, meta: dict = None) -> str:\n",
        "    parts = [clean_text(subject), clean_text(body)]\n",
        "    if meta:\n",
        "        # Optional light features from metadata\n",
        "        for k in ('product','plan','region'):\n",
        "            if k in meta and meta[k]:\n",
        "                parts.append(f\"{k}:{str(meta[k]).lower()}\")\n",
        "    return \" \".join(p for p in parts if p)\n"
      ],
      "metadata": {
        "id": "2vmGvtoVUCEj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastAPI"
      ],
      "metadata": {
        "id": "n2V00lHFV50C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "df = load_dataset(\"interneuronai/companyx_customer_support_ticket_routing_distilbert_dataset\")\n",
        "\n",
        "train_df = df['train'].to_pandas()\n",
        "\n",
        "# Display the first 6 rows\n",
        "print(train_df.head(6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRG_whh7tQyo",
        "outputId": "e1c72753-f2dd-47cd-f0e4-8e36b49fc007"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0.1  Unnamed: 0  \\\n",
            "0             0           0   \n",
            "1             1           1   \n",
            "2             2           2   \n",
            "3             3           3   \n",
            "4             4           4   \n",
            "5             5           5   \n",
            "\n",
            "                                                text        label  text:  \n",
            "0  Mənim keçmişim proqram təminatı üzrə ixtisasla...     software    NaN  \n",
            "1  Mən UNIX sistem idarəçiliyində güclü təcrübəyə...   consulting    NaN  \n",
            "2  Proqram tərtibatçısı kimi kiçik skriptlərdən t...  development    NaN  \n",
            "3  Proqram təminatı mütəxəssisi olaraq veb proqra...     software    NaN  \n",
            "4  Men UNIX sisdem idarechiliyine dikked yediren ...   consulting    NaN  \n",
            "5  Prokram derdibadchısı kimi men daxili sisdemle...  development    NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df.apply(lambda r: combine_fields(\n",
        "    r.get('subject',''), r.get('body',''),\n",
        "    {'product': r.get('product'), 'plan': r.get('plan'), 'region': r.get('region')}\n",
        "), axis=1)\n"
      ],
      "metadata": {
        "id": "DYIOtQdXslrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------\n",
        "# 3) Split data\n",
        "# -----------------------\n",
        "X = df['text'].values\n",
        "y_urgency = df['urgency_label'].values      # e.g., high|normal|low\n",
        "y_topic   = df['topic_label'].values        # e.g., billing|bug|feature_request|account\n",
        "\n",
        "Xtr, Xte, yU_tr, yU_te = train_test_split(X, y_urgency, test_size=0.2, random_state=42, stratify=y_urgency)\n",
        "_,   _,   yT_tr, yT_te = train_test_split(X, y_topic,   test_size=0.2, random_state=42, stratify=y_topic)\n",
        "\n",
        "# -----------------------\n",
        "# 4) Vectorizer (fast & small)\n",
        "# -----------------------\n",
        "vectorizer = HashingVectorizer(\n",
        "    n_features=2**20,          # tune vs. memory\n",
        "    ngram_range=(1,2),\n",
        "    norm='l2',\n",
        "    alternate_sign=False,      # improves Logistic/SGD stability\n",
        "    lowercase=False            # we already lowercased\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5) Models\n",
        "# -----------------------\n",
        "# Urgency: imbalanced => linear SVM (SGD hinge/log) with class_weight\n",
        "urg_base = SGDClassifier(\n",
        "    loss='log_loss',           # probabilistic; good for calibration\n",
        "    class_weight='balanced',\n",
        "    alpha=1e-5,\n",
        "    max_iter=2000,\n",
        "    tol=1e-3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "urg_clf = Pipeline([\n",
        "    ('vec', vectorizer),\n",
        "    ('clf', CalibratedClassifierCV(urg_base, cv=3))  # better probs for thresholding\n",
        "])\n",
        "\n",
        "# Topic: usually multi-class, often more balanced\n",
        "top_base = LogisticRegression(\n",
        "    class_weight=None,\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "top_clf = Pipeline([\n",
        "    ('vec', vectorizer),\n",
        "    ('clf', top_base)\n",
        "])\n",
        "\n",
        "# -----------------------\n",
        "# 6) Train\n",
        "# -----------------------\n",
        "urg_clf.fit(Xtr, yU_tr)\n",
        "top_clf.fit(Xtr, yT_tr)\n",
        "\n",
        "# -----------------------\n",
        "# 7) Evaluate\n",
        "# -----------------------\n",
        "print(\"=== URGENCY ===\")\n",
        "yU_pred = urg_clf.predict(Xte)\n",
        "print(classification_report(yU_te, yU_pred))\n",
        "\n",
        "print(\"=== TOPIC ===\")\n",
        "yT_pred = top_clf.predict(Xte)\n",
        "print(classification_report(yT_te, yT_pred))\n",
        "\n",
        "# Optional: tune a custom threshold for \"high\" urgency to boost recall at fixed precision\n",
        "def find_best_threshold(pipe, X_valid: List[str], y_valid: List[str], positive_label='high', target_precision=0.80):\n",
        "    proba = pipe.predict_proba(X_valid)\n",
        "    # proba[:, idx_of_high]\n",
        "    classes = pipe.named_steps['clf'].classes_\n",
        "    idx = int(np.where(classes == positive_label)[0])\n",
        "    scores = proba[:, idx]\n",
        "    # sweep thresholds\n",
        "    best = (0.5, 0.0, 0.0)  # (thr, precision, recall)\n",
        "    for thr in np.linspace(0.2, 0.9, 36):\n",
        "        pred = np.where(scores >= thr, positive_label, 'not_high')\n",
        "        # compute precision/recall\n",
        "        tp = np.sum((pred == positive_label) & (np.array(y_valid) == positive_label))\n",
        "        fp = np.sum((pred == positive_label) & (np.array(y_valid) != positive_label))\n",
        "        fn = np.sum((pred != positive_label) & (np.array(y_valid) == positive_label))\n",
        "        precision = tp / (tp + fp + 1e-9)\n",
        "        recall    = tp / (tp + fn + 1e-9)\n",
        "        if precision >= target_precision and recall > best[2]:\n",
        "            best = (thr, precision, recall)\n",
        "    return best\n",
        "\n",
        "thr, p, r = find_best_threshold(urg_clf, Xte, yU_te, positive_label='high', target_precision=0.85)\n",
        "print(f\"Chosen high-urgency threshold: {thr:.2f} (precision={p:.2f}, recall={r:.2f})\")\n",
        "\n",
        "# -----------------------\n",
        "# 8) Persist\n",
        "# -----------------------\n",
        "Path(\"artifacts\").mkdir(exist_ok=True)\n",
        "joblib.dump(urg_clf, \"artifacts/urgency_clf.joblib\")\n",
        "joblib.dump(top_clf, \"artifacts/topic_clf.joblib\")\n",
        "with open(\"artifacts/urgency_threshold.json\", \"w\") as f:\n",
        "    json.dump({\"high_threshold\": thr}, f)"
      ],
      "metadata": {
        "id": "yoB3nXfbUCS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hNDeu1-vUCWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2-UQfbCUCZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPpjxWPeUCcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4c2y_guUCfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import re\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------\n",
        "# 1) Minimal text cleaner\n",
        "# -----------------------\n",
        "URL_RE = re.compile(r'https?://\\S+')\n",
        "WS_RE  = re.compile(r'\\s+')\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(' ', s)\n",
        "    s = s.replace('\\n', ' ')\n",
        "    s = WS_RE.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def combine_fields(subject: str, body: str, meta: dict = None) -> str:\n",
        "    parts = [clean_text(subject), clean_text(body)]\n",
        "    if meta:\n",
        "        # Optional light features from metadata\n",
        "        for k in ('product','plan','region'):\n",
        "            if k in meta and meta[k]:\n",
        "                parts.append(f\"{k}:{str(meta[k]).lower()}\")\n",
        "    return \" \".join(p for p in parts if p)\n",
        "\n",
        "# -----------------------\n",
        "# 2) Load your dataset\n",
        "# -----------------------\n",
        "# Expect a CSV with: subject, body, urgency_label, topic_label, (optional) product,plan,region\n",
        "df = pd.read_csv(\"tickets.csv\")\n",
        "\n",
        "df['text'] = df.apply(lambda r: combine_fields(\n",
        "    r.get('subject',''), r.get('body',''),\n",
        "    {'product': r.get('product'), 'plan': r.get('plan'), 'region': r.get('region')}\n",
        "), axis=1)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Split data\n",
        "# -----------------------\n",
        "X = df['text'].values\n",
        "y_urgency = df['urgency_label'].values      # e.g., high|normal|low\n",
        "y_topic   = df['topic_label'].values        # e.g., billing|bug|feature_request|account\n",
        "\n",
        "Xtr, Xte, yU_tr, yU_te = train_test_split(X, y_urgency, test_size=0.2, random_state=42, stratify=y_urgency)\n",
        "_,   _,   yT_tr, yT_te = train_test_split(X, y_topic,   test_size=0.2, random_state=42, stratify=y_topic)\n",
        "\n",
        "# -----------------------\n",
        "# 4) Vectorizer (fast & small)\n",
        "# -----------------------\n",
        "vectorizer = HashingVectorizer(\n",
        "    n_features=2**20,          # tune vs. memory\n",
        "    ngram_range=(1,2),\n",
        "    norm='l2',\n",
        "    alternate_sign=False,      # improves Logistic/SGD stability\n",
        "    lowercase=False            # we already lowercased\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5) Models\n",
        "# -----------------------\n",
        "# Urgency: imbalanced => linear SVM (SGD hinge/log) with class_weight\n",
        "urg_base = SGDClassifier(\n",
        "    loss='log_loss',           # probabilistic; good for calibration\n",
        "    class_weight='balanced',\n",
        "    alpha=1e-5,\n",
        "    max_iter=2000,\n",
        "    tol=1e-3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "urg_clf = Pipeline([\n",
        "    ('vec', vectorizer),\n",
        "    ('clf', CalibratedClassifierCV(urg_base, cv=3))  # better probs for thresholding\n",
        "])\n",
        "\n",
        "# Topic: usually multi-class, often more balanced\n",
        "top_base = LogisticRegression(\n",
        "    class_weight=None,\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "top_clf = Pipeline([\n",
        "    ('vec', vectorizer),\n",
        "    ('clf', top_base)\n",
        "])\n",
        "\n",
        "# -----------------------\n",
        "# 6) Train\n",
        "# -----------------------\n",
        "urg_clf.fit(Xtr, yU_tr)\n",
        "top_clf.fit(Xtr, yT_tr)\n",
        "\n",
        "# -----------------------\n",
        "# 7) Evaluate\n",
        "# -----------------------\n",
        "print(\"=== URGENCY ===\")\n",
        "yU_pred = urg_clf.predict(Xte)\n",
        "print(classification_report(yU_te, yU_pred))\n",
        "\n",
        "print(\"=== TOPIC ===\")\n",
        "yT_pred = top_clf.predict(Xte)\n",
        "print(classification_report(yT_te, yT_pred))\n",
        "\n",
        "# Optional: tune a custom threshold for \"high\" urgency to boost recall at fixed precision\n",
        "def find_best_threshold(pipe, X_valid: List[str], y_valid: List[str], positive_label='high', target_precision=0.80):\n",
        "    proba = pipe.predict_proba(X_valid)\n",
        "    # proba[:, idx_of_high]\n",
        "    classes = pipe.named_steps['clf'].classes_\n",
        "    idx = int(np.where(classes == positive_label)[0])\n",
        "    scores = proba[:, idx]\n",
        "    # sweep thresholds\n",
        "    best = (0.5, 0.0, 0.0)  # (thr, precision, recall)\n",
        "    for thr in np.linspace(0.2, 0.9, 36):\n",
        "        pred = np.where(scores >= thr, positive_label, 'not_high')\n",
        "        # compute precision/recall\n",
        "        tp = np.sum((pred == positive_label) & (np.array(y_valid) == positive_label))\n",
        "        fp = np.sum((pred == positive_label) & (np.array(y_valid) != positive_label))\n",
        "        fn = np.sum((pred != positive_label) & (np.array(y_valid) == positive_label))\n",
        "        precision = tp / (tp + fp + 1e-9)\n",
        "        recall    = tp / (tp + fn + 1e-9)\n",
        "        if precision >= target_precision and recall > best[2]:\n",
        "            best = (thr, precision, recall)\n",
        "    return best\n",
        "\n",
        "thr, p, r = find_best_threshold(urg_clf, Xte, yU_te, positive_label='high', target_precision=0.85)\n",
        "print(f\"Chosen high-urgency threshold: {thr:.2f} (precision={p:.2f}, recall={r:.2f})\")\n",
        "\n",
        "# -----------------------\n",
        "# 8) Persist\n",
        "# -----------------------\n",
        "Path(\"artifacts\").mkdir(exist_ok=True)\n",
        "joblib.dump(urg_clf, \"artifacts/urgency_clf.joblib\")\n",
        "joblib.dump(top_clf, \"artifacts/topic_clf.joblib\")\n",
        "with open(\"artifacts/urgency_threshold.json\", \"w\") as f:\n",
        "    json.dump({\"high_threshold\": thr}, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "KqemIxA0UCix",
        "outputId": "f92ffa5c-23e7-47fa-bed7-6fc664044609"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tickets.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2401478756.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# -----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Expect a CSV with: subject, body, urgency_label, topic_label, (optional) product,plan,region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tickets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m df['text'] = df.apply(lambda r: combine_fields(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tickets.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Real-time inference API (FastAPI)"
      ],
      "metadata": {
        "id": "7XTMJfTKxFVw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFhO_ohtw_MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "import json\n",
        "import joblib\n",
        "import time\n",
        "import uvicorn\n",
        "from pathlib import Path\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Literal\n",
        "\n",
        "from train import clean_text, combine_fields  # reuse the exact same cleaning\n",
        "\n",
        "ARTIFACTS = Path(\"artifacts\")\n",
        "urgency_clf = joblib.load(ARTIFACTS / \"urgency_clf.joblib\")\n",
        "topic_clf   = joblib.load(ARTIFACTS / \"topic_clf.joblib\")\n",
        "with open(ARTIFACTS / \"urgency_threshold.json\") as f:\n",
        "    URG_CONF = json.load(f)\n",
        "HIGH_THR = float(URG_CONF.get(\"high_threshold\", 0.5))\n",
        "\n",
        "app = FastAPI(title=\"Ticket Triage API\", version=\"1.0.0\")\n",
        "\n",
        "class TicketIn(BaseModel):\n",
        "    subject: str\n",
        "    body: str\n",
        "    product: Optional[str] = None\n",
        "    plan: Optional[str] = None\n",
        "    region: Optional[str] = None\n",
        "\n",
        "class TriageOut(BaseModel):\n",
        "    urgency: Literal[\"high\",\"normal\",\"low\"]\n",
        "    topic: str\n",
        "    confidence: float\n",
        "    route_queue: str\n",
        "    latency_ms: float\n",
        "\n",
        "def route_decision(urgency: str, topic: str) -> str:\n",
        "    # Simple rules; make this a config in production\n",
        "    if urgency == \"high\":\n",
        "        return f\"{topic}_priority_queue\"\n",
        "    if topic in {\"billing\"}:\n",
        "        return \"billing_queue\"\n",
        "    return f\"{topic}_queue\"\n",
        "\n",
        "@app.post(\"/triage\", response_model=TriageOut)\n",
        "def triage(ticket: TicketIn):\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    text = combine_fields(ticket.subject, ticket.body, {\n",
        "        \"product\": ticket.product,\n",
        "        \"plan\": ticket.plan,\n",
        "        \"region\": ticket.region\n",
        "    })\n",
        "\n",
        "    # Predict topic (argmax)\n",
        "    topic = topic_clf.predict([text])[0]\n",
        "    # Predict urgency via calibrated probabilities + threshold for \"high\"\n",
        "    urg_proba = urgency_clf.predict_proba([text])[0]\n",
        "    urg_classes = urgency_clf.named_steps['clf'].classes_\n",
        "    probs = dict(zip(urg_classes, urg_proba))\n",
        "    # choose label with max probability\n",
        "    urgency = max(probs, key=probs.get)\n",
        "\n",
        "    # Apply custom rule: if \"high\" prob >= HIGH_THR -> force high\n",
        "    if 'high' in probs and probs['high'] >= HIGH_THR:\n",
        "        urgency = 'high'\n",
        "\n",
        "    # Confidence (of chosen label)\n",
        "    confidence = float(probs.get(urgency, max(probs.values())))\n",
        "\n",
        "    route = route_decision(urgency, topic)\n",
        "    latency = (time.perf_counter() - t0) * 1000.0\n",
        "\n",
        "    return TriageOut(\n",
        "        urgency=urgency,\n",
        "        topic=topic,\n",
        "        confidence=round(confidence, 4),\n",
        "        route_queue=route,\n",
        "        latency_ms=round(latency, 2)\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # local dev\n",
        "    uvicorn.run(\"app:triage\", host=\"0.0.0.0\", port=8000, reload=True)\n"
      ],
      "metadata": {
        "id": "alickl5bw_PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sxM5btpw_SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bF8eiYJDw_VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7bCofZOw_YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMkC-z_ww_bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}